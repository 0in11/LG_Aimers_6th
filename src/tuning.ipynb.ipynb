{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def objective(trial, X, y, n_splits=5):\n",
    "    # 기본 파라미터 설정\n",
    "    params = {\n",
    "        \"random_state\": 42,\n",
    "        \"verbosity\": 0,\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"tree_method\": \"gpu_hist\",\n",
    "        \"device\": \"cuda:0\"  # 최신 XGBoost 버전 호환성\n",
    "    }\n",
    "\n",
    "    # 최적화할 하이퍼파라미터 설정\n",
    "    params.update({\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),\n",
    "        'scale_pos_weight': trial.suggest_float('scale_pos_weight', 1, (len(y) - sum(y)) / sum(y))\n",
    "    })\n",
    "\n",
    "    # 교차 검증 설정\n",
    "    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "\n",
    "    for train_idx, val_idx in cv.split(X, y):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        # DMatrix 생성\n",
    "        dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "        dval = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "        # 모델 학습\n",
    "        evals_result = {}\n",
    "        model = xgb.train(\n",
    "            params,\n",
    "            dtrain,\n",
    "            num_boost_round=10000,\n",
    "            evals=[(dval, 'validation')],\n",
    "            early_stopping_rounds=50,\n",
    "            evals_result=evals_result,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "\n",
    "        # 검증 세트에 대한 예측 및 성능 평가\n",
    "        y_pred = model.predict(dval)\n",
    "        fold_score = roc_auc_score(y_val, y_pred)\n",
    "        scores.append(fold_score)\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "def tune_xgboost(X, y, n_trials=100, n_splits=5):\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        study_name=\"xgboost_optimization\",\n",
    "        pruner=optuna.pruners.MedianPruner(\n",
    "            n_startup_trials=5,\n",
    "            n_warmup_steps=5,\n",
    "            interval_steps=3\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # 최적화 실행\n",
    "    study.optimize(\n",
    "        lambda trial: objective(trial, X, y, n_splits),\n",
    "        n_trials=n_trials,\n",
    "        timeout=None,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "\n",
    "    # 최적의 하이퍼파라미터 출력\n",
    "    print(\"\\n=== Best Trial ===\")\n",
    "    print(f\"Value (AUC): {study.best_value:.4f}\")\n",
    "    print(\"Best parameters:\")\n",
    "    for key, value in study.best_params.items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "    \n",
    "    return study.best_params, study.best_value\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X = train_processed.drop(columns=[\"임신 성공 여부\"])\n",
    "    y = train_processed[\"임신 성공 여부\"]\n",
    "    \n",
    "    # 메모리 최적화 (대용량 데이터를 위한 옵션)\n",
    "    def optimize_dtypes(df):\n",
    "        \"\"\"데이터프레임의 데이터 타입을 최적화하여 메모리 사용량 감소\"\"\"\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == 'float64':\n",
    "                df[col] = df[col].astype('float32')\n",
    "            elif df[col].dtype == 'int64':\n",
    "                df[col] = df[col].astype('int32')\n",
    "        return df\n",
    "\n",
    "    X = optimize_dtypes(X)\n",
    "    \n",
    "    # 데이터 정보 출력\n",
    "    print(f\"데이터 크기: {X.shape[0]} 행, {X.shape[1]} 열\")\n",
    "    \n",
    "    # 하이퍼파라미터 최적화 실행\n",
    "    best_params, best_score = tune_xgboost(X, y, n_trials=100)\n",
    "    \n",
    "    # 최적 파라미터를 파일로 저장\n",
    "    import json\n",
    "    with open('best_xgboost_params.json', 'w') as f:\n",
    "        json.dump(best_params, f, indent=4)\n",
    "    \n",
    "    print(\"최적 파라미터가 'best_xgboost_params.json'에 저장되었습니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
